{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwoOSGuckRHS"
      },
      "outputs": [],
      "source": [
        "Question1: Define the z-statistic and explain its relationship to the standard normal distribution. How is the z-statistic used in hypothesis testing?\n",
        "\n",
        "The z-statistic is a standardized score, meaning it transforms a data point from its original scale to a unitless metric that reflects its position relative to the mean of the distribution. This transformation allows comparisons across different datasets and distributions by converting data to a common scale.\n",
        "\n",
        "Relationship to the Standard Normal Distribution\n",
        "The z-statistic relates to the standard normal distribution, which is a normal distribution with a mean of 0 and a standard deviation of 1. When a dataset is transformed into z-scores, the distribution of z-scores will resemble a standard normal distribution. A z-score of 0 corresponds to the mean of the distribution, and z-scores of ±1, ±2, and ±3 correspond to 1, 2, and 3 standard deviations away from the mean, respectively.\n",
        "\n",
        "Use of the Z-Statistic in Hypothesis Testing\n",
        "In hypothesis testing, the z-statistic is used to determine the probability of observing a sample statistic, like a sample mean, assuming the null hypothesis is true. The steps typically include:\n",
        "\n",
        "1. Formulate the hypotheses (null and alternative).\n",
        "2. Calculate the z-score for the sample statistic based on the population parameters.\n",
        "3. Find the p-value associated with the z-score using the standard normal distribution. This p-value indicates the probability of observing a value as extreme as the sample statistic.\n",
        "4. Compare the p-value to a significance level (often 0.05) to decide whether to reject the null hypothesis.\n",
        "\n",
        "If the z-score lies in the critical region (e.g., beyond ±1.96 for a 95% confidence level), we reject the null hypothesis, concluding that the sample provides enough evidence against it. This process is common in tests of proportions and means, where population parameters are known or approximated."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question2 : What is a p-value, and how is it used in hypothesis testing? What does it mean if the p-value is very small (e.g., 0.01)?\n",
        "\n",
        "A p-value is the probability of observing test results at least as extreme as the actual observed results, assuming the null hypothesis is true. In hypothesis testing, the p-value helps determine whether the evidence in the data is strong enough to reject the null hypothesis.\n",
        "\n",
        "How the P-Value is Used in Hypothesis Testing\n",
        "1. Set a Significance Level (α): Before conducting the test, researchers choose a significance level, often 0.05, as the threshold for deciding whether to reject the null hypothesis.\n",
        "2. Calculate the P-Value: The p-value is computed based on the test statistic derived from the sample data.\n",
        "3. Compare the P-Value with α:\n",
        "- If the p-value is less than or equal to α, we reject the null hypothesis. This suggests that the observed result is statistically significant and unlikely to have occurred by chance.\n",
        "- If the p-value is greater than α, we do not reject the null hypothesis. This indicates that there is insufficient evidence to conclude that the observed result is significant.\n",
        "\n",
        "Interpretation of a Very Small P-Value (e.g., 0.01)\n",
        "A p-value of 0.01 means that there is only a 1% chance of observing data as extreme as, or more extreme than, the actual results if the null hypothesis were true. In practical terms:\n",
        "1. A very small p-value (e.g., 0.01 or lower) provides strong evidence against the null hypothesis.\n",
        "2. If the p-value is 0.01, it suggests that the observed effect is statistically significant and likely not due to random chance."
      ],
      "metadata": {
        "id": "8ZuEosNblhng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question3: Compare and contrast the binomial and Bernoulli distributions.\n",
        "\n",
        "Key differences between the binomial and Bernoulli distributions:\n",
        "1. Number of Trials: The Bernoulli distribution models a single trial, while the binomial distribution models multiple (specifically,n) independent trials.\n",
        "2. Parameters: The Bernoulli distribution has one parameter, p, while the binomial distribution has two parameters, n and p.\n",
        "3. Applications: The Bernoulli distribution is used for experiments with one trial, while the binomial distribution is used to analyze experiments with multiple independent trials.\n",
        "\n",
        "\n",
        "Relationship\n",
        "The binomial distribution is essentially the sum of multiple independent Bernoulli trials. For instance, if you perform n Bernoulli trials with probability p, the total number of successes will follow a binomial distribution with parameters n and p.\n"
      ],
      "metadata": {
        "id": "ut4vm3zgmxgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question 4: Under what conditions is the binomial distribution used, and how does it relate to the Bernoulli distribution?\n",
        "\n",
        "he binomial distribution is used under specific conditions where the outcome of each trial can be classified as either a \"success\" or \"failure.\" The conditions that justify using a binomial distribution are as follows:\n",
        "\n",
        "Conditions for Using the Binomial Distribution:\n",
        "1. Fixed Number of Trials (n): There is a fixed, finite number of trials (denoted as n).\n",
        "2. Two Possible Outcomes per Trial: Each trial has only two outcomes, typically classified as \"success\" (e.g., heads in a coin flip) and \"failure\" (e.g., tails).\n",
        "3. Constant Probability of Success (p): The probability of success, p, remains constant for each trial.\n",
        "4. Independence of Trials: Each trial is independent, meaning the outcome of one trial does not affect the outcome of another.\n",
        "\n",
        "When these conditions are met, the binomial distribution can be used to model the number of successes in n trials.\n",
        "\n",
        "Relationship to the Bernoulli Distribution:\n",
        "The binomial distribution is directly related to the Bernoulli distribution:\n",
        "\n",
        "1. The Bernoulli distribution models the outcome of a single trial with only two possible outcomes, success (1) or failure (0).\n",
        "2. The binomial distribution can be seen as the sum of multiple independent Bernoulli trials.\n",
        "\n",
        "If we repeat a Bernoulli experiment n times with a success probability p, the total number of successes in these n trials follows a binomial distribution with parameters n (number of trials) and p (probability of success in each trial).\n",
        "\n",
        "Example:\n",
        "If you flip a fair coin 10 times, the probability of heads in each flip is 0.5 (a Bernoulli trial with p=0.5). The binomial distribution with n=10 and p=0.5 would describe the probability distribution for getting a certain number of heads (e.g., exactly 5 heads) in those 10 flips.\n",
        "\n",
        "In essence, the binomial distribution represents the sum of the outcomes of independent Bernoulli trials."
      ],
      "metadata": {
        "id": "QK2XIpZmo2pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question5: What are the key properties of the Poisson distribution, and when is it appropriate to use this distribution?\n",
        "\n",
        "The Poisson distribution is a discrete probability distribution that describes the likelihood of a given number of events occurring within a fixed interval of time, space, or other domains, given that these events occur independently and at a constant average rate.\n",
        "\n",
        "Key Properties of the Poisson Distribution\n",
        "1. Parameter (λ): The Poisson distribution is characterized by a single parameter, λ, which represents the average rate of occurrence (e.g., the average number of events per time unit). This parameter is both the mean and variance of the distribution.\n",
        "Mean: E(X)=λ\n",
        "Variance: Var(X)=λ\n",
        "\n",
        "2. Probability Mass Function (PMF): For a random variable X that follows a Poisson distribution, the probability of observingk events in a given interval is:\n",
        "P(X=k)= λ k e −λ/ k!\n",
        "where\n",
        "k is a non-negative integer (0, 1, 2, …), and e≈2.718 is Euler's number.\n",
        "\n",
        "3. Discrete Nature: The Poisson distribution only takes on integer values (0, 1, 2, …) because it models counts of events.\n",
        "\n",
        "4. Skewness: The Poisson distribution is positively skewed, especially for smaller values of λ. As λ increases, the distribution becomes more symmetric and resembles a normal distribution due to the Central Limit Theorem.\n",
        "\n",
        "When to Use the Poisson Distribution:\n",
        "The Poisson distribution is appropriate when:\n",
        "\n",
        "1. Events Occur Randomly: The events must occur randomly and independently.\n",
        "2. Constant Average Rate: Events must occur at a constant average rate (λ) over the interval being studied.\n",
        "3. Rare Events: The Poisson distribution is often used for rare events where the probability of multiple occurrences in a very short interval is low.\n",
        "4. Non-Overlapping Intervals: The number of events occurring in disjoint intervals should be independent of each other.\n",
        "\n",
        "Common Applications of the Poisson Distribution:\n",
        "The Poisson distribution is widely used in scenarios where we count occurrences over time or space. Examples include:\n",
        "1. The number of calls received by a call center per hour.\n",
        "2. The number of emails arriving in an inbox per minute.\n",
        "3. The number of cars passing through a toll booth in an hour.\n",
        "4. The occurrence of rare diseases in a population over a year."
      ],
      "metadata": {
        "id": "p7VeqnwNsI6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question6: Define the terms \"probability distribution\" and \"probability density function\" (PDF). How does a PDF differ from a probability mass function (PMF)?\n",
        "\n",
        "Probability Distribution\n",
        "A probability distribution describes how the values of a random variable are distributed across possible outcomes, indicating the likelihood of each possible value. Probability distributions can be either discrete or continuous:\n",
        "1. Discrete Probability Distribution: Defined for discrete random variables (e.g., counts), it describes the probability of each specific outcome.\n",
        "2. Continuous Probability Distribution: Defined for continuous random variables (e.g., measurements), it describes the likelihood of outcomes within a range rather than specific values.\n",
        "\n",
        "Probability Density Function (PDF)\n",
        "A probability density function (PDF) is a function that specifies the likelihood of a continuous random variable falling within a particular range of values, rather than at any exact value (since the probability of a continuous variable taking any single value is technically zero).\n",
        "\n",
        "\n",
        "Difference Between PDF and PMF\n",
        "1. Domain: The PDF applies to continuous random variables, whereas the PMF applies to discrete random variables.\n",
        "2. Interpretation: For a PMF, p(x) represents the probability of exactly x. For a PDF, f(x) does not represent probability directly but rather the density at x; probabilities are found by integrating the PDF over an interval.\n",
        "3. Calculation of Probability: For a discrete random variable with a PMF, probabilities are the sums of individual probabilities. For a continuous random variable with a PDF, probabilities are areas under the curve calculated by integration."
      ],
      "metadata": {
        "id": "_JAIeZh7tme-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question7: Explain the Central Limit Theorem (CLT) with example.\n",
        "\n",
        "The Central Limit Theorem (CLT) is a fundamental principle in statistics that states that the sampling distribution of the sample mean (or sum) of a sufficiently large number of independent and identically distributed (i.i.d.) random variables will approximate a normal distribution, regardless of the original distribution of the population.\n",
        "\n",
        "Example of the Central Limit Theorem:\n",
        "Suppose a factory produces light bulbs, and the lifespan of each light bulb follows a skewed distribution with a mean lifespan of 500 hours and a standard deviation of 100 hours. If we take a small sample of, say, 5 light bulbs, the average lifespan in this sample might not closely follow the normal distribution due to the skewed nature of the original distribution.\n",
        "\n",
        "However, if we take larger samples, say, of 50 light bulbs each, and calculate the average lifespan for each sample, the distribution of these sample means will approximate a normal distribution, centered around the population mean of 500 hours, even though the population distribution is skewed. The larger the sample size, the closer the distribution of the sample means will resemble a normal distribution, due to the Central Limit Theorem.\n",
        "\n",
        "Importance of the Central Limit Theorem:\n",
        "The CLT allows us to make inferences about population parameters using the sample mean, even if the population distribution is not normal. This is particularly valuable because it underpins many statistical methods, such as confidence intervals and hypothesis testing, which rely on normality assumptions."
      ],
      "metadata": {
        "id": "UnR4a3HMuVWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question8: Compare z-scores and t-scores. When should you use a z-score, and when should a t-score be applied instead?\n",
        "\n",
        "Key Differences Between Z-Scores and T-Scores\n",
        "1. Sample Size: Z-scores are used for larger samples, while t-scores are used for smaller samples.\n",
        "2. Population Standard Deviation: Z-scores require the population standard deviation, while t-scores are applied when only the sample standard deviation is available.\n",
        "3. Distribution: Z-scores follow a standard normal distribution, whereas t-scores follow a t-distribution with n−1 degrees of freedom, which accounts for additional variability due to a smaller sample size. The t-distribution is wider (has heavier tails) than the normal distribution, but as the sample size increases, the t-distribution approaches the normal distribution.\n",
        "\n",
        "Example:\n",
        "If you want to estimate the average height of a population and:\n",
        "\n",
        "1. You have a large sample (e.g., 50 people) and know the population standard deviation, use a z-score.\n",
        "2. You have a small sample (e.g., 15 people) and do not know the population standard deviation, use a t-score."
      ],
      "metadata": {
        "id": "nILtcEmWuxe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question9: Given a sample mean of 105, a population mean of 100, a standard deviation of 15, and a sample\n",
        "size of 25, calculate the z-score and p-value. Based on a significance level of 0.05, do you reject or fail to\n",
        "reject the null hypothesis?\n",
        "\n",
        "Task: Write Python code to calculate the z-score and p-value for the given data.\n",
        "\n",
        "Objective: Apply the formula for the z-score and interpret the p-value for hypothesis testing.\n",
        "\n",
        "\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "# Given data\n",
        "sample_mean = 105\n",
        "population_mean = 100\n",
        "standard_deviation = 15\n",
        "sample_size = 25\n",
        "significance_level = 0.05\n",
        "\n",
        "# Calculate z-score\n",
        "z_score = (sample_mean - population_mean) / (standard_deviation / (sample_size ** 0.5))\n",
        "\n",
        "# Calculate p-value (two-tailed test)\n",
        "p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
        "\n",
        "z_score, p_value\n",
        "\n",
        "\n",
        "Decision:\n",
        "Since the p-value (0.096) is greater than the significance level (0.05), we fail to reject the null hypothesis. This suggests that there is not enough evidence at the 5% significance level to conclude that the sample mean significantly differs from the population mean. ​\n",
        "\n"
      ],
      "metadata": {
        "id": "orK096eTvPQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question10: Simulate a binomial distribution with 10 trials and a probability of success of 0.6 using Python.\n",
        "Generate 1,000 samples and plot the distribution. What is the expected mean and variance?\n",
        "\n",
        "Task: Use Python to generate the data, plot the distribution, and calculate the mean and variance.\n",
        "\n",
        "Objective: Understand the properties of a binomial distribution and verify them through simulation.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for the binomial distribution\n",
        "n_trials = 10       # Number of trials\n",
        "p_success = 0.6     # Probability of success\n",
        "n_samples = 1000    # Number of samples\n",
        "\n",
        "# Generate 1000 samples from a binomial distribution\n",
        "binomial_samples = np.random.binomial(n_trials, p_success, n_samples)\n",
        "\n",
        "# Calculate the mean and variance of the simulated data\n",
        "simulated_mean = np.mean(binomial_samples)\n",
        "simulated_variance = np.var(binomial_samples)\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(binomial_samples, bins=range(0, n_trials + 2), edgecolor='black', density=True)\n",
        "plt.title(\"Simulated Binomial Distribution (n=10, p=0.6)\")\n",
        "plt.xlabel(\"Number of Successes\")\n",
        "plt.ylabel(\"Frequency (Normalized)\")\n",
        "plt.xticks(range(0, n_trials + 1))\n",
        "plt.show()\n",
        "\n",
        "# Expected mean and variance of the binomial distribution\n",
        "expected_mean = n_trials * p_success\n",
        "expected_variance = n_trials * p_success * (1 - p_success)\n",
        "\n",
        "simulated_mean, simulated_variance, expected_mean, expected_variance\n"
      ],
      "metadata": {
        "id": "BqxmqIGNvfzf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}